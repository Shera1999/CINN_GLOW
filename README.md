# Conditional Invertible Neural Networks for Galaxy‑Cluster Merger History

Welcome to **cINN‑Clusters**, a reproducible pipeline that predicts hidden merger properties of galaxy clusters from observable X‑ray quantities using a *Conditional Invertible Neural Network* (cINN).
The repository also includes a benchmark MLP ensemble, extensive posterior‑diagnostic plots, and a feature‑sensitivity analysis.

---

## 🌐 Directory Structure

```
CINN_GLOW/
├── scalar/
│   ├── data_filter.py              # Pre-processing from raw → scaled CSVs
│   ├── model.py                    # cINN architecture (FrEIA + PyTorch)
│   ├── train_cinn.py               # Training script for the flow model
│   ├── mlp_baseline.py             # 7‑member MLP ensemble baseline
│   ├── processed_data/             # Generated by data_filter.py
│   │   ├── X.csv | Y.csv | meta.csv
│   │   ├── obs_scaler.pkl | tar_scaler.pkl
│   │   └── … (intermediate files)
│   ├── best_cluster_cinn.pt        # Saved model checkpoint (after training)
│   ├── 1.posterior_distrubution.png
│   ├── 2.prediction_performance1.png
│   ├── 2.prediction_performance2.png
│   ├── 3.uncertainities.png
│   ├── 4.cross_correlations.png
│   └── 5.sensitivity_analysis.png
├── README.md
└── requirements.txt
```

All code lives inside the \`\` sub‑directory; feel free to reorganise later (e.g., move plotting scripts into their own folder), but the README assumes the structure above for now.

---

## ⚙️  Installation

```bash
# clone & enter
git clone <repo‑url> cinn_project
cd cinn_project

# create env & install deps
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

`requirements.txt` lists:

```
pandas numpy torch scikit‑learn joblib matplotlib FrEIA
```

CUDA is auto‑detected if available.

---

## 📊 Data Preparation

```bash
python data_filter.py \
   --obs_csv  path/to/observables1.csv \
   --unobs_csv path/to/unobservables1.csv \
   --out_dir  processed_data
```

*Creates* `processed_data/` with scaled feature/target matrices plus fitted `StandardScaler` pickles.
`data_filter.py` reproduces the exact train/val/test split used everywhere else by fixing `random_state` to **42**.

---

## 🧩 Training the cINN

```bash
python train_cinn.py \
   --processed_dir processed_data \
   --epochs 250
```

*Outputs* `best_cluster_cinn.pt` (saved whenever validation NLL improves).

### Hyper‑parameters

* `hidden_dim=128`, `n_blocks=12`, `clamp=2.0` (Glow‑style affine coupling blocks)
* Optimiser: Adam, `lr=2 × 10⁻³`

---

## 🏷️  Baseline MLP Ensemble & Sensitivity

```bash
python mlp_baseline.py --processed_dir processed_data
```

1. Trains **7** independent MLPs (phase‑1 MSE → phase‑2 MAE).
2. Writes median test predictions to `mlp_test_pred.npy` and ground truth to `mlp_test_true.npy`.
3. Produces `sensitivity_matrix.csv` used in Fig. 5.

---

## 📈 Generating Figures

All plotting scripts assume:

* `best_cluster_cinn.pt` exists in project root.
* `processed_data/` contains the pre‑processed CSVs & scalers.

Run any script directly, e.g.

```bash
python plotting/posterior_distribution.py            # Fig. 1  prior v posterior grid
python plotting/plot_posteriors_all_targets.py       # Fig. 2a heat‑maps
python plotting/plot_map_and_error_vs_truth.py       # Fig. 2b truth v MAP
python plotting/plot_error_vs_std_scatter.py         # Fig. 3  calibration
python plotting/plot_pairwise_all_three.py           # Fig. 4  pairwise correlations
python plotting/sensitivity_heatmap.py               # Fig. 5  feature sensitivity
```

Each script saves a high‑resolution PNG (and commented‑out PDF) in the repo root with intuitive file names:

```
posterior_distribution.png
2.prediction_performance1.png
2.prediction_performance2.png
3.uncertainities.png
4.cross_correlations.png
5.sensitivity_analysis.png
```

---

## 🖼️  Figure Gallery

Below is a quick visual preview of every figure produced by the pipeline.  Each image is down‑scaled for the README; click to view full resolution.

| #       | Visualisation                                                 | Insight                                                                                                                                            |             |                                                               |
| ------- | ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ------------------------------------------------------------- |
| **1**   | <img src="scalar/1.posterior_distrubution.png" width="260"/>  | **Prior vs Posterior** — KDE curves for prior (grey), posterior (blue), MAP (gold) and ground‑truth (red) for every cluster and target (Figure 1). |             |                                                               |
| **2 a** | <img src="scalar/2.prediction_performance1.png" width="260"/> | **Posterior Heat‑maps** — How predicted posteriors shift relative to prior bins across all targets (Figure 2a).                                    |             |                                                               |
| **2 b** | <img src="scalar/2.prediction_performance2.png" width="260"/> | **MAP & Error Trends** — Ground‑truth vs MAP (top) and absolute error trends (bottom) with percentile bands (Figure 2b).                           |             |                                                               |
| **3**   | <img src="scalar/3.uncertainities.png" width="260"/>          | **Uncertainty Calibration** —                                                                                                                      | MAP − truth | versus posterior σ with Gaussian reference curves (Figure 3). |
| **4**   | <img src="scalar/4.cross_correlations.png" width="260"/>      | **Cross‑correlations** — Pairwise scatter of truth, posterior samples and MAP predictions for every target pair (Figure 4).                        |             |                                                               |
| **5**   | <img src="scalar/5.sensitivity_analysis.png" width="260"/>    | **Feature Sensitivity** — Δ MAE heat‑map showing which observables most influence each target (Figure 5).                                          |             |                                                               |

---

## 🤝 Contributing & Issues Contributing & Issues

Pull requests and bug reports are welcome.
Please open an issue for questions or feature requests.

---

## 📄 License

MIT License © 2025 Your Name
