# Conditional Invertible Neural Networks for Galaxy‑Cluster Merger History

Welcome to **cINN‑Clusters**, a reproducible pipeline that predicts hidden merger properties of galaxy clusters from observable X‑ray quantities using a *Conditional Invertible Neural Network* (cINN).
The repository also includes a benchmark MLP ensemble, extensive posterior‑diagnostic plots, and a feature‑sensitivity analysis.

---

## Directory Structure

```
CINN_GLOW/
├── scalar/
│   ├── data_filter.py              # Pre-processing from raw → scaled CSVs
│   ├── model.py                    # cINN architecture (FrEIA + PyTorch)
│   ├── train_cinn.py               # Training script for the flow model
│   ├── mlp_baseline.py             # 7‑member MLP ensemble baseline
│   ├── processed_data/             # Generated by data_filter.py
│   │   ├── X.csv | Y.csv | meta.csv
│   │   ├── obs_scaler.pkl | tar_scaler.pkl
│   │   └── … (intermediate files)
│   ├── best_cluster_cinn.pt        # Saved model checkpoint (after training)
│   ├── 1.posterior_distrubution.png
│   ├── 2.prediction_performance1.png
│   ├── 2.prediction_performance2.png
│   ├── 3.uncertainities.png
│   ├── 4.cross_correlations.png
│   └── 5.sensitivity_analysis.png
├── README.md
└── requirements.txt
```

All code lives inside the \`\` sub‑directory; feel free to reorganise later (e.g., move plotting scripts into their own folder), but the README assumes the structure above for now.

---

##  Installation

```bash
# clone & enter
git clone <repo‑url> cinn_project
cd cinn_project

# create env & install deps
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

`requirements.txt` lists:

```
pandas numpy torch scikit‑learn joblib matplotlib FrEIA
```

CUDA is auto‑detected if available.

---

## Data Preparation

```bash
python data_filter.py \
   --obs_csv  path/to/observables1.csv \
   --unobs_csv path/to/unobservables1.csv \
   --out_dir  processed_data
```

*Creates* `processed_data/` with scaled feature/target matrices plus fitted `StandardScaler` pickles.
`data_filter.py` reproduces the exact train/val/test split used everywhere else by fixing `random_state` to **42**.

---

## Training the cINN

```bash
python train_cinn.py \
   --processed_dir processed_data \
   --epochs 250
```

*Outputs* `best_cluster_cinn.pt` (saved whenever validation NLL improves).

### Hyper‑parameters

* `hidden_dim=128`, `n_blocks=12`, `clamp=2.0` (Glow‑style affine coupling blocks)
* Optimiser: Adam, `lr=2 × 10⁻³`

---

##  Baseline MLP Ensemble & Sensitivity

```bash
python mlp_baseline.py --processed_dir processed_data
```

1. Trains **7** independent MLPs (phase‑1 MSE → phase‑2 MAE).
2. Writes median test predictions to `mlp_test_pred.npy` and ground truth to `mlp_test_true.npy`.
3. Produces `sensitivity_matrix.csv` used in Fig. 5.

---

## Generating Figures

All plotting scripts assume:

* `best_cluster_cinn.pt` exists in project root.
* `processed_data/` contains the pre‑processed CSVs & scalers.

Run any script directly, e.g.

```bash
python plotting/posterior_distribution.py            # Fig. 1  prior v posterior grid
python plotting/plot_posteriors_all_targets.py       # Fig. 2a heat‑maps
python plotting/plot_map_and_error_vs_truth.py       # Fig. 2b truth v MAP
python plotting/plot_error_vs_std_scatter.py         # Fig. 3  calibration
python plotting/plot_pairwise_all_three.py           # Fig. 4  pairwise correlations
python plotting/sensitivity_heatmap.py               # Fig. 5  feature sensitivity
```

Each script saves a high‑resolution PNG (and commented‑out PDF) in the repo root with intuitive file names:

```
posterior_distribution.png
2.prediction_performance1.png
2.prediction_performance2.png
3.uncertainities.png
4.cross_correlations.png
5.sensitivity_analysis.png
```

---

##  Figure Gallery

<<<<<<< HEAD
Below are full‑width previews of every figure produced by the pipeline.  Click any image for the high‑resolution version.
=======
| #  | File                           | Insight                                                                        
| -- | ------------------------------ | ------------------------------------------------------------------------------ 
| 1  | posterior\_distribution.png    | Side‑by‑side prior/posterior comparison with MAP + truth per cluster.          
| 2a | 2.prediction\_performance1.png | Heat‑maps of how posteriors shift relative to prior bins across targets.       
| 2b | 2.prediction\_performance2.png | MAP accuracy & error distribution as a function of ground‑truth value.         
| 3  | 3.uncertainities.png           | Checks correlation between predicted σ and actual                              
| 4  | 4.cross\_correlations.png      | Joint distributions (truth, posterior, MAP) for every target pair.             
| 5  | 5.sensitivity\_analysis.png    | Which observables most influence MAE for each target (ensemble ablation test). 
>>>>>>> ce63deb (scalar_glow)

### 1  Prior vs Posterior (Figure 1)

![Prior vs Posterior](scalar/1.posterior_distrubution.png)
*Side‑by‑side KDE curves of the population prior (grey dashed), model posterior (blue), MAP estimate (gold), and ground truth (red) for every cluster and target.*

---

### 2a  Posterior Heat‑maps (Figure 2a)

![Posterior Heat‑maps](scalar/2.prediction_performance1.png)
*2‑D heat‑maps of prior bin → posterior bin counts, overlaid with median (solid) and 10th/90th percentile (dashed) lines for each target.*

---

### 2b  MAP & Error Trends (Figure 2b)

![MAP & Error Trends](scalar/2.prediction_performance2.png)
*Top row: Ground‑truth vs MAP predictions.  Bottom row: Absolute error vs truth (symlog scale) with 16th/84th percentile bands.*

---

### 3  Uncertainty Calibration (Figure 3)

![Uncertainty Calibration](scalar/3.uncertainities.png)
*Scatter of |MAP − truth| versus posterior σ, including Gaussian reference curves and binned 68th/95th percentile error lines.*

---

### 4  Cross‑correlations (Figure 4)

![Cross‑correlations](scalar/4.cross_correlations.png)
*Staircase grid of pairwise scatter plots showing joint distributions of truth (red), posterior samples (light‑blue), and MAP predictions (mustard) for every target pair.*

---

### 5  Feature Sensitivity (Figure 5)

![Feature Sensitivity](scalar/5.sensitivity_analysis.png)
*Log‑scaled Δ MAE heat‑map indicating how omitting each observable affects prediction error for each target.*

---
