# CINN_GLOW
# Conditional Invertible Neural Networks for Galaxy‑Cluster Merger History

Welcome to **cINN‑Clusters**, a reproducible pipeline that predicts hidden merger properties of galaxy clusters from observable quantities using a *Conditional Invertible Neural Network* (cINN).
The repository also includes a benchmark MLP ensemble, extensive posterior‑diagnostic plots, and a feature‑sensitivity analysis.

---

## Directory Structure

```
CINN_GLOW/
├── scalar/
│   ├── data_filter.py              # Pre-processing from raw → scaled CSVs
│   ├── model.py                    # cINN architecture (FrEIA + PyTorch)
│   ├── train_cinn.py               # Training script for the flow model
│   ├── mlp_baseline.py             # 7‑member MLP ensemble baseline
│   ├── processed_data/             # Generated by data_filter.py
│   │   ├── X.csv | Y.csv | meta.csv
│   │   ├── obs_scaler.pkl | tar_scaler.pkl
│   │   └── … (intermediate files)
│   ├── best_cluster_cinn.pt        # Saved model checkpoint (after training)
│   ├── 1.posterior_distrubution.png
│   ├── 2.prediction_performance1.png
│   ├── 2.prediction_performance2.png
│   ├── 3.uncertainities.png
│   ├── 4.cross_correlations.png
│   └── 5.sensitivity_analysis.png
├── README.md
└── requirements.txt
```

All code lives inside the \`\` sub‑directory; feel free to reorganise later (e.g., move plotting scripts into their own folder), but the README assumes the structure above for now.

---

##  Installation

```bash
# clone & enter
git clone 
cd cinn_project

# create env & install deps
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

`requirements.txt` lists:

```
pandas numpy torch scikit‑learn joblib matplotlib FrEIA
```

CUDA is auto‑detected if available.

---

## Data Preparation

```bash
python data_filter.py \
   --obs_csv  path/to/observables1.csv \
   --unobs_csv path/to/unobservables1.csv \
   --out_dir  processed_data
```

*Creates* `processed_data/` with scaled feature/target matrices plus fitted `StandardScaler` pickles.
`data_filter.py` reproduces the exact train/val/test split used everywhere else by fixing `random_state` to **42**.

---

## Training the cINN

```bash
python train_cinn.py \
   --processed_dir processed_data \
   --epochs 250
```

*Outputs* `best_cluster_cinn.pt` (saved whenever validation NLL improves).

### Hyper‑parameters

* `hidden_dim=128`, `n_blocks=12`, `clamp=2.0` (Glow‑style affine coupling blocks)
* Optimiser: Adam, `lr=2 × 10⁻³`

---

##  Baseline MLP Ensemble & Sensitivity

```bash
python mlp_baseline.py --processed_dir processed_data
```

1. Trains **7** independent MLPs (phase‑1 MSE → phase‑2 MAE).
2. Writes median test predictions to `mlp_test_pred.npy` and ground truth to `mlp_test_true.npy`.
3. Produces `sensitivity_matrix.csv` used in Fig. 5.

---

## Generating Figures

All plotting scripts assume:

* `best_cluster_cinn.pt` exists in project root.
* `processed_data/` contains the pre‑processed CSVs & scalers.

Run any script directly, e.g.

```bash
python plotting/posterior_distribution.py            # Fig. 1  prior v posterior grid
python plotting/plot_posteriors_all_targets.py       # Fig. 2a heat‑maps
python plotting/plot_map_and_error_vs_truth.py       # Fig. 2b truth v MAP
python plotting/plot_error_vs_std_scatter.py         # Fig. 3  calibration
python plotting/plot_pairwise_all_three.py           # Fig. 4  pairwise correlations
python plotting/sensitivity_heatmap.py               # Fig. 5  feature sensitivity
```

Each script saves a high‑resolution PNG (and commented‑out PDF) in the repo root with intuitive file names:

```
posterior_distribution.png
2.prediction_performance1.png
2.prediction_performance2.png
3.uncertainities.png
4.cross_correlations.png
5.sensitivity_analysis.png
```

---

##  Figure Gallery (quick preview)

| #  | File                           | Insight                                                                        |       |                            |
| -- | ------------------------------ | ------------------------------------------------------------------------------ | ----- | -------------------------- |
| 1  | posterior\_distribution.png    | Side‑by‑side prior/posterior comparison with MAP + truth per cluster.          |       |                            |
| 2a | 2.prediction\_performance1.png | Heat‑maps of how posteriors shift relative to prior bins across targets.       |       |                            |
| 2b | 2.prediction\_performance2.png | MAP accuracy & error distribution as a function of ground‑truth value.         |       |                            |
| 3  | 3.uncertainities.png           | Checks correlation between predicted σ and actual                              | error | (uncertainty calibration). |
| 4  | 4.cross\_correlations.png      | Joint distributions (truth, posterior, MAP) for every target pair.             |       |                            |
| 5  | 5.sensitivity\_analysis.png    | Which observables most influence MAE for each target (ensemble ablation test). |       |                            |



