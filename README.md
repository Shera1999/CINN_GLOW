# Conditional Invertible Neural Networks for Galaxyâ€‘Cluster Merger History

Welcome to **cINNâ€‘Clusters**, a reproducible pipeline that predicts hidden merger properties of galaxy clusters from observable Xâ€‘ray quantities using a *Conditional Invertible Neural Network* (cINN).
The repository also includes a benchmark MLP ensemble, posteriorâ€‘diagnostic plots, and a featureâ€‘sensitivity analysis.

### Brief Primer: GLOW Coupling Blocks & Conditional INNs

**GLOW-style affine coupling blocks**
GLOW (Kingma & Dhariwal 2018) is a flow-based generative model that builds an **invertible** mapping between data `y` and a latent variable `z ~ ğ’©(0,I)`.
A *coupling block* splits the input channels into two halves:

1. `yâ‚` (frozen pass-through)
2. `yâ‚‚` (transformed)

A shallow **sub-network** `s,t = f(yâ‚, x)` produces scale and shift vectors, and the transformation is

```
yâ‚‚â€² = yâ‚‚ âŠ™ exp(clamp Â· s) + t                 # forward
yâ‚‚  = (yâ‚‚â€² âˆ’ t) âŠ™ exp(âˆ’clamp Â· s)            # inverse
log |det J| = Î£ clamp Â· s                     # tractable Jacobian
```

Because half the channels pass through unchanged, the mapping is exactly invertible, fast in both directions, and its Jacobian determinant is cheap to computeâ€”perfect for maximum-likelihood training.

A **random permutation** (or 1Ã—1 convolution) is inserted between blocks so every dimension eventually influences every other.

---

**cINN â€“ Conditional Invertible Neural Network**
A *conditional* INN learns a bijection **p(y | x)** by *conditioning* every coupling block on auxiliary inputs `x` (our observables / embeddings).
The forward pass converts targets `y` into latent `z`, and the inverse pass samples from the posterior `p(y | x)` by drawing `z` from a standard normal and running the network backward.

Key advantages:

* **Exact, analytic log-likelihood** â†’ stable training.
* **Efficient sampling** â†’ thousands of posterior draws per object in milliseconds.
* **Invertibility** â†’ no information loss; uncertainty is naturally modelled.

---

### Dual-Input Design in This Repository

| Pipeline                          | Conditioning Vector **x**                                                                                             | Folder                         |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------ |
| **Scalar-only**                   | 7 physical observables (CSV `X.csv`)                                                                                  | `scalar/`                      |
| **Scalar + Representation Space** | â€¢ Same 7 observables **plus**<br>â€¢ A learnedâ€image embedding (â‰ˆ D\_emb â‰ˆ 256â€“512) from the https://github.com/Shera1999/contrastive-learning respository which includes few self-supervised learning methods that are performed on X-ray maps of galaxy clusters.

*Both* pipelines predict the same 6 merger-history targets (`Y.csv`) and share identical cINN architecture and training logic; they differ only in **input dimensionality** and therefore have separate checkpoints (`best_cluster_cinn.pt`, `best_cluster_cinn_combined.pt`) and plotting notebooks.


---

## Directory Structure

```
CINN_GLOW/
â”œâ”€â”€ scalar/
â”‚   â”œâ”€â”€ data_filter.py              # Pre-processing from raw â†’ scaled CSVs
â”‚   â”œâ”€â”€ model.py                    # cINN architecture (FrEIA + PyTorch)
â”‚   â”œâ”€â”€ train_cinn.py               # Training script for the flow model
â”‚   â”œâ”€â”€ mlp_baseline.py             # MLP ensemble baseline
â”‚   â”œâ”€â”€ processed_data/             # Generated by data_filter.py
â”‚   â”‚   â”œâ”€â”€ X.csv | Y.csv | meta.csv
â”‚   â”‚   â”œâ”€â”€ obs_scaler.pkl | tar_scaler.pkl
â”‚   â”‚   â””â”€â”€ â€¦ (intermediate files)
â”‚   â”œâ”€â”€ 1.posterior_distrubution.ipynb
â”‚   â”œâ”€â”€ 2.prediction_performance1.ipynb
â”‚   â”œâ”€â”€ 2.prediction_performance2.ipynb
â”‚   â”œâ”€â”€ 3.uncertainities.ipynb
â”‚   â”œâ”€â”€ 4.cross_correlations.ipynb
â”‚   â””â”€â”€ 5.sensitivity_analysis.ipynb
â”œâ”€â”€ scalar+representation_space/  # pipeline using scalars + learned embeddings
â”‚   â”œâ”€â”€ data_filter.py            # shared pre-processing
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train_cinn_combined.py
â”‚   â”œâ”€â”€ mlp_baseline.py
â”‚   â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ embeddings.npy
â”‚   â”œâ”€â”€ filenames.npy
â”‚   â”œâ”€â”€ best_cluster_cinn_combined.pt
â”‚   â”œâ”€â”€ 1.posterior_distribution.ipynb
â”‚   â”œâ”€â”€ 2.prediction_performance1.ipynb
â”‚   â”œâ”€â”€ 2.prediction_performance2.pnipynbg
â”‚   â””â”€â”€ 3.sensitivity_analysis.ipynb
â”œâ”€â”€ README.md

```

All code lives inside the \`\` subâ€‘directory; feel free to reorganise later (e.g., move plotting scripts into their own folder), but the README assumes the structure above for now.

---

##  Installation

```bash
# clone & enter
git clone https://github.com/Shera1999/CINN_GLOW.git
cd cinn_project

# create env & install deps
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

`requirements.txt` lists:

```
pandas numpy torch scikitâ€‘learn joblib matplotlib FrEIA
```

CUDA is autoâ€‘detected if available.

---

## Data Preparation

```bash
python data_filter.py \
   --obs_csv  path/to/observables1.csv \
   --unobs_csv path/to/unobservables1.csv \
   --out_dir  processed_data
```

*Creates* `processed_data/` with scaled feature/target matrices plus fitted `StandardScaler` pickles.
`data_filter.py` reproduces the exact train/val/test split used everywhere else by fixing `random_state` to **42**.

---

## Training the cINN

```bash
python train_cinn.py \
   --processed_dir processed_data \
   --epochs 250
```

*Outputs* `best_cluster_cinn.pt` (saved whenever validation NLL improves).

### Hyperâ€‘parameters

* `hidden_dim=128`, `n_blocks=12`, `clamp=2.0` (Glowâ€‘style affine coupling blocks)
* Optimiser: Adam, `lr=2â€¯Ã—â€¯10â»Â³`

---

##  Baseline MLP Ensemble & Sensitivity

```bash
python mlp_baseline.py --processed_dir processed_data
```

1. Trains **7** independent MLPs (phaseâ€‘1 MSE â†’ phaseâ€‘2 MAE).
2. Writes median test predictions to `mlp_test_pred.npy` and ground truth to `mlp_test_true.npy`.
3. Produces `sensitivity_matrix.csv` used in Fig.â€¯5.

---

## Generating Figures

All plotting scripts assume:

* `best_cluster_cinn.pt` exists in project root.
* `processed_data/` contains the preâ€‘processed CSVs & scalers.

Run any script inside the jupyter notebook files:

```bash
1.posterior_distribution.ipynb       # Fig.Â 1  priorÂ vÂ posterior grid
2.prediction_performance.ipynb       # Fig.Â 2a heatâ€‘maps, Fig.Â 2b truthÂ vÂ MAP
3.uncertainities.ipynb               # Fig.Â 3  calibration
4.cross_correlation.ipynb            # Fig.Â 4  pairwise correlations
5.sensitivity_analysis.ipynb         # Fig.Â 5  feature sensitivity
```

Each script saves a highâ€‘resolution PNG (and commentedâ€‘out PDF) in the repo root with intuitive file names:

```
posterior_distribution.png
2.prediction_performance1.png
2.prediction_performance2.png
3.uncertainities.png
4.cross_correlations.png
5.sensitivity_analysis.png
```

---

##  Figure Gallery

=======
| #  | File                           | Insight                                                                        
| -- | ------------------------------ | ------------------------------------------------------------------------------ 
| 1  | 1.posterior\_distribution.png    | Sideâ€‘byâ€‘side prior/posterior comparison with MAP + truth per cluster.          
| 2a | 2.prediction\_performance1.png | Heatâ€‘maps of how posteriors shift relative to prior bins across targets.       
| 2b | 2.prediction\_performance2.png | MAP accuracy & error distribution as a function of groundâ€‘truth value.         
| 3  | 3.uncertainities.png           | Checks correlation between predicted Ïƒ and actual                              
| 4  | 4.cross\_correlations.png      | Joint distributions (truth, posterior, MAP) for every target pair.             
| 5  | 5.sensitivity\_analysis.png    | Which observables most influence MAE for each target (ensemble ablation test). 

### 1Â Â PriorÂ vsÂ Posterior (FigureÂ 1)

![Prior vs Posterior](scalar/1.posterior_distrubution.png)
*Sideâ€‘byâ€‘side KDE curves of the population prior (grey dashed), model posterior (blue), MAP estimate (gold), and ground truth (red) for every cluster and target.*

---

### 2aÂ Â Posterior Heatâ€‘maps (FigureÂ 2a)

![Posterior Heatâ€‘maps](scalar/2.prediction_performance1.png)
*2â€‘D heatâ€‘maps of prior bin â†’ posterior bin counts, overlaid with median (solid) and 10th/90th percentile (dashed) lines for each target.*

---

### 2bÂ Â MAP & Error Trends (FigureÂ 2b)

![MAP & Error Trends](scalar/2.prediction_performance2.png)
*Top row: Groundâ€‘truth vs MAP predictions.  Bottom row: Absolute error vs truth (symlog scale) with 16th/84th percentile bands.*

---

### 3Â Â Uncertainty Calibration (FigureÂ 3)

![Uncertainty Calibration](scalar/3.uncertainities.png)
*Scatter of |MAP âˆ’ truth| versus posterior Ïƒ, including Gaussian reference curves and binned 68th/95th percentile error lines.*

---

### 4Â Â Crossâ€‘correlations (FigureÂ 4)

![Crossâ€‘correlations](scalar/4.cross_correlations.png)
*Staircase grid of pairwise scatter plots showing joint distributions of truth (red), posterior samples (lightâ€‘blue), and MAP predictions (mustard) for every target pair.*

---

### 5Â Â Feature Sensitivity (FigureÂ 5)

![Feature Sensitivity](scalar/5.sensitivity_analysis.png)
*Logâ€‘scaled Î”Â MAE heatâ€‘map indicating how omitting each observable affects prediction error for each target.*

---

### ScalarsÂ + Representationâ€‘Space Pipeline

Figures below show results when **learned embeddings** (representation space) which is a result from https://github.com/Shera1999/contrastive-learning, are concatenated with scalar observables as the cINN condition.

#### 1Â Â Prior vs Posterior (Combined)

![Prior vs Posterior â€“ Combined](scalar+representation_space/1.posterior_distribution.png)
*Posterior vs prior KDEs when the model is conditioned on both scalar features and learned image embeddings.*

---

#### 2aÂ Â Posterior Heatâ€‘maps (Combined)

![Posterior Heatâ€‘maps â€“ Combined](scalar+representation_space/2.prediction_performance1.png)
*Heatâ€‘maps showing how posterior samples distribute across prior bins for the combinedâ€‘input model.*

---

#### 2bÂ Â MAP & Error Trends (Combined)

![MAP & Error Trends â€“ Combined](scalar+representation_space/2.prediction_performance2.png)
*Truth vs MAP and error trends for the combinedâ€‘input model.*

---

#### 3Â Â Feature Sensitivity (Combined)

![Feature Sensitivity â€“ Combined](scalar+representation_space/3.sensitivity_analysis.png)
*Î”â€¯MAE heatâ€‘map after ablation in the combinedâ€‘input MLP ensemble.*

---